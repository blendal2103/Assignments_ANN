{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd741f0",
   "metadata": {},
   "source": [
    "### Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "- Perceptron is basic unit of Artificial Neural Network. Activation functions act like magic sauce or Ingredient to the Artificial Neural Network .\n",
    "- Perceptron is not able to capture the non linear patterns. So it enables Perceptron to capture the non linear pattern in a Data.\n",
    "- By using Activation function ,Perceptron is able predict output on linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862249a0",
   "metadata": {},
   "source": [
    "### Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "    These are the some common types of activation function :\n",
    "        1. Sigmoid Activation Function\n",
    "        2. Rectified Linear Unit (Relu)\n",
    "        3. Leaky Relu \n",
    "        4. Softmax Activation Function\n",
    "        5. Hyperbolic Tangent Activation Function\n",
    "        6. Parametric Relu\n",
    "        7. Randomized Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d40ed",
   "metadata": {},
   "source": [
    "### Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "    Activation function plays a great role in a training process of a model and overall performance of a Neural Network. They adds the non-linearity to a network.\n",
    "\n",
    "- Non Linearity : It adds non linearity by which our model is able to learn the non-linear relationship between the independent and dependent features.\n",
    "-  Training Speed : The activation function also have some impact in training process .It slightly increases the training sped. The activation function is easy for computation which enables for fast forward and backward passes.\n",
    "- Regularization : Certain activation functions can act as a form of regularization by preventing neurons from completely deactivating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2732d",
   "metadata": {},
   "source": [
    "### Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "        Sigmpoid activation function is a non linear function which is used in logistic regression.\n",
    "        Activation function that transforms input values between the range 0 to 1.\n",
    "        It's formula is :\n",
    "\n",
    "             Sigmoid Function = 1/(1+e^(-x))\n",
    "\n",
    "- Working : When input is given to the Sigmoid function it transforms the input between the range of 0 to 1. When our input is more negative , it transforms it to 0. When our input is more positive, it transforms it to 1.\n",
    "\n",
    "- Advantages :\n",
    "   1. Output Range : This is a main advantage of Sigmoid function as it has ouput between the range 0 to 1 . It enables it to use for binary classification problem.\n",
    "   2. Smootheness : The Sigmoid function is differentiable at every point. This property is very useful for optimization in neural networks.\n",
    "\n",
    "- Disadvantage :\n",
    "   1. Vanishing Gradient : As the input becomes very large or becomes very small , the derivative reaches to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a2e1d",
   "metadata": {},
   "source": [
    "### Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "    Rectified Linear Unit is widely used no linear fuinctions in Neural Network .\n",
    "    It's formula is :\n",
    "\n",
    "    f(x)=max(0,x)\n",
    "\n",
    "    It has a output range between zero to Infinty .\n",
    "\n",
    "- Difference Between Sigmoid and Relu :\n",
    "   1. Main difference between Sigmoid and Relu is their output Range. Sigmoid has a zero to one and Relu has zero to infinity.\n",
    "   2. Sigmoid suffers from a vanishing gradient for high positive and negative inputs. The Relu has mitigated it .\n",
    "   3. Relu is efficient for Computation rather than sgmoid function.\n",
    "   4. Relu suffers the Dead Neuron for negative inputs . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f205e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13f5eff",
   "metadata": {},
   "source": [
    "### Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Here are some advantages of Relu over Sigmoid :\n",
    "1. Mitigates the problem of Vanishing Gradient : In case of Relu , the gradient becomes close to zero as input goes to high positive or negative . This is called the vanishing Gradient. \n",
    "2. Computationally Efficient : The Relu is easy compute rather than Sigmoid function due to its simple operations.\n",
    "3. Faster Convergence : Due to mitigation of Vanishing Gradient problem, the convergence in Relu is faster than Sigmoid.\n",
    "4. Relu Faces the Dead Neuron problem which can be mitigated by several varients of Relu ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c02be",
   "metadata": {},
   "source": [
    "### Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "    Leaky Relu is a Varient of Rectified Linear Unit which has mitigated the dead relu problem.\n",
    "    It has the same formula for postive input but it has different formula for negative input.\n",
    "        A new parameter  α (alpha) is introduced in the formula for negative inputs. The parameter has a value 0.01 for a leaky relu.\n",
    "\n",
    "     The new formula is as follows: \n",
    "\n",
    " -     f(x) = max(x,0) for x > 0\n",
    " -            αx  for x <= 0\n",
    "\n",
    " For  Leaky relu , the ouput range is [-∞,∞] .For the negative input values the output is not stucked to zero. So by this way, dead relu problem is mitigated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dabedc",
   "metadata": {},
   "source": [
    "### Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "    The Softmax is a activation function which is most widely used in output layer of Neural Networks.\n",
    "    When we have a multiclass classification prioblem then there softmax is used as it returns probability of input belongs to category .\n",
    "\n",
    "    In multiclass classification tasks, softmax is used in conjunction with the categorical cross-entropy loss function. The predicted probabilities are compared to the true class labels to calculate the loss, which guides the network's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f4628",
   "metadata": {},
   "source": [
    "### Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "Hyperbolic Tangent Function  is similar to sigmoid Activation function. It has a different ouput range . The tanh function tansforms he input in the range between -1 to 1.\n",
    "\n",
    "The tanh function outputs are centred around zero makes it advantageous for mitigating the gradient related problem .\n",
    "\n",
    "Tanh have a derivative range 1 rather than sigmoid it has 0.25.\n",
    "The graph have same shape for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73096e9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
